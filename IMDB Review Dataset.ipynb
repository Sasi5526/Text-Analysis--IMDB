{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning and predicting Sentiment Analyses for Reviews of IMDB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Library & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import textblob\n",
    "stop = stopwords.words('english')\n",
    "from nltk.corpus import opinion_lexicon\n",
    "pos_list=set(opinion_lexicon.positive())\n",
    "neg_list=set(opinion_lexicon.negative())\n",
    "from nltk.tokenize import treebank\n",
    "tokenizer = treebank.TreebankWordTokenizer()\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from afinn import Afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import LancasterStemmer\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "ls =nltk.stem.LancasterStemmer()\n",
    "import requests \n",
    "\n",
    "\n",
    "contractions_dict = {\n",
    "    'didn\\'t': 'did not',\n",
    "    'don\\'t': 'do not',\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"cant\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"dont\" : \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i had\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"im\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they had\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }\n",
    "\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, s)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def strip_html_tags():\n",
    "    soup = BeautifulSoup(content,\"html.parser\")\n",
    "    [s.extract for s in soup(['iframe','script'])]\n",
    "    stripped_text = soup.get_text() \n",
    "    stripped_text=re.sub('[\\r|\\n|\\r\\n]+','\\n',stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD',text).encode('ascii','ignore').decode('utf-8','ignore')\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text, remove_digits = False):\n",
    "    patterns = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern,\"\",text)\n",
    "    return text\n",
    "\n",
    "def simple_stemmers(text,stemmer = ps):\n",
    "    text = \" \".join([stemmer.stem(word)for word in text.split()])\n",
    "    return text\n",
    "\n",
    "def expand_contraction(text):\n",
    "    return contraction.fix(text)\n",
    "\n",
    "def spacy_lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ !='-PRON-' else word.text for word in text])\n",
    "\n",
    "def remove_stopwords(text, is_lower_case = False, stopwords = None):\n",
    "    if not stopwords:\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens=[token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [tokens for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "         filtered_tokens = [tokens for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_tokens = ' '.join(filtered_tokens)\n",
    "    return filtered_tokens    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def text_pre_processor(text,html_strip=True, accented_char=True,contraction_expansion=True,text_lower_case=True,\n",
    "                       text_stemming=False, text_lemmatization=True,special_char_removal=True, remove_digits=True,\n",
    "                       stopword_removal=True, stopword_list=None): \n",
    "    #strip HTML\n",
    "    if html_strip:\n",
    "        text=strip_html_tags(text)\n",
    "    \n",
    "    #remove extra newlines(often might be present in really noisy text)\n",
    "    text = text.translate(text.maketrans(\"\\n\\t\\r\",\" \"))\n",
    "\n",
    "    #remove accented character\n",
    "    if accented_char_removal:\n",
    "        text = removal_accented_chars(text)\n",
    "    \n",
    "   #expand contraction\n",
    "    if contraction_expansion:\n",
    "         text = spacy_lemmatize_text(text)\n",
    "    \n",
    "   #Lemmatize text\n",
    "    if text_lemmatization:\n",
    "         text = spacy_lemmatize_text(text)\n",
    "    \n",
    "   #remove special characters and \\or digits\n",
    "    if special_char_removal:\n",
    "   #insert space between special characters to isolate them\n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        text = special_char_pattern.sub(\"\\\\1 \", text)\n",
    "        text = remove_special_characters(text, remove_digits = remove_digits)\n",
    "    \n",
    "   #stem text\n",
    "    if text_stemming and not text_lemmatization:\n",
    "         text = simple_stemming(text)\n",
    "    \n",
    "   #lowercase the text\n",
    "    if text_lower_case:\n",
    "         text = text.lower()\n",
    "\n",
    "   #remove stopwords\n",
    "    if stopword_removal:\n",
    "         text = remove_stopwords(text,is_lower_case = text_lower_case,stopwords=stopword_list)\n",
    "    \n",
    "   #remove extra whitespace\n",
    "    text = re.sub(' +', ' ',text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def corpus_pre_processor(corpus):\n",
    "    norm_corpus = []\n",
    "    for doc in tqdm.tqdm(corpus):\n",
    "        norm_corpus.append(text_pre_processor(doc))\n",
    "    return norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "import re\n",
    "contractions_dict = {\n",
    "    'didn\\'t': 'did not',\n",
    "    'don\\'t': 'do not',\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"cant\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"dont\" : \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i had\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"im\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they had\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }\n",
    "\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, s)\n",
    "\n",
    "\n",
    "def pre_process_corpus(docs):\n",
    "    norm_docs = []\n",
    "    for doc in tqdm.tqdm(docs):\n",
    "        doc = strip_html_tags(doc)\n",
    "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "        doc = doc.lower()\n",
    "        doc = remove_accented_chars(doc)\n",
    "        doc = expand_contractions(doc)\n",
    "        # lower case and remove special characters\\whitespaces\n",
    "        doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        doc = doc.strip()  \n",
    "        norm_docs.append(doc)\n",
    "  \n",
    "    return norm_docs\n",
    "\n",
    "\n",
    "import tqdm\n",
    "def corpus_pre_processor(corpus):\n",
    "    norm_corpus = []\n",
    "    for doc in tqdm.tqdm(corpus):\n",
    "        norm_corpus.append(normalize_document(doc))\n",
    "    return norm_corpus\n",
    "def normalize_corpus(doc):\n",
    "    #Lowercase, remove special char \\whitespace\n",
    "    #remove stopwords\n",
    "    #expand contraction\n",
    "    words= word_tokenize(doc)\n",
    "    doc = \" \".join([word.lower() for word in words if word not in stop])\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]','',doc,re.I|re.A)\n",
    "    doc =doc.strip()\n",
    "    doc = expand_contractions(doc)\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def corpus_pre_processor(corpus):\n",
    "    norm_corpus = []\n",
    "    for doc in tqdm.tqdm(corpus):\n",
    "        norm_corpus.append(normalize_document(doc))\n",
    "    return norm_corpus\n",
    "def normalize_corpus(doc):\n",
    "    #Lowercase, remove special char \\whitespace\n",
    "    #remove stopwords\n",
    "    #expand contraction\n",
    "    words= word_tokenize(doc)\n",
    "    doc = \" \".join([word.lower() for word in words if word not in stop])\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]','',doc,re.I|re.A)\n",
    "    doc =doc.strip()\n",
    "    doc = expand_contractions(doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"D:\\\\sasi\\\\study\\\\ML\\\\ML project\\\\imdb-dataset-of-50k-movie-reviews\\\\IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "    #Lowercase, remove special char \\whitespace\n",
    "    #remove stopwords\n",
    "    #expand contraction\n",
    "    words= word_tokenize(doc)\n",
    "    doc = \" \".join([word.lower() for word in words if word not in stop])\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]','',doc,re.I|re.A)\n",
    "    doc =doc.strip()\n",
    "    doc = expand_contractions(doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sasim\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "#split the review into sentence\n",
    "data=dataset[['review','index']]\n",
    "data.rename(columns ={'index':'INDEX'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>INDEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  INDEX\n",
       "0  One of the other reviewers has mentioned that ...      0\n",
       "1  A wonderful little production. <br /><br />The...      1\n",
       "2  I thought this was a wonderful way to spend ti...      2\n",
       "3  Basically there's a family where a little boy ...      3\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...      4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "data['split'] = data['review'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>INDEX</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[One of the other reviewers has mentioned that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "      <td>[A wonderful little production., &lt;br /&gt;&lt;br /&gt;T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>2</td>\n",
       "      <td>[I thought this was a wonderful way to spend t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[Basically there's a family where a little boy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Petter Mattei's \"Love in the Time of Money\" i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  INDEX  \\\n",
       "0  One of the other reviewers has mentioned that ...      0   \n",
       "1  A wonderful little production. <br /><br />The...      1   \n",
       "2  I thought this was a wonderful way to spend ti...      2   \n",
       "3  Basically there's a family where a little boy ...      3   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...      4   \n",
       "\n",
       "                                               split  \n",
       "0  [One of the other reviewers has mentioned that...  \n",
       "1  [A wonderful little production., <br /><br />T...  \n",
       "2  [I thought this was a wonderful way to spend t...  \n",
       "3  [Basically there's a family where a little boy...  \n",
       "4  [Petter Mattei's \"Love in the Time of Money\" i...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split=data.set_index('INDEX').split.apply(pd.Series).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split=data.set_index('INDEX').split.apply(pd.Series).stack().reset_index(level=0).rename(columns={0:'Reviews'})\n",
    "data_split.reset_index(level=0,inplace=True)\n",
    "data_split.rename(columns={'INDEX':'review_no','index':'sentence'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>review_no</th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>They are right, as this is exactly what happen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Trust me, this is not a show for the faint hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>This show pulls no punches with regards to dru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Its is hardcore, in the classic use of the wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>It focuses mainly on Emerald City, an experime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Em City is home to many..Aryans, Muslims, gang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>Forget pretty pictures painted for mainstream ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>The first episode I ever saw struck me as so n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Not just violence, but injustice (crooked guar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;The filming technique is very unas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;The actors are extremely well chos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>You can truly see the seamless editing guided ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A masterful production about one of the great ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentence  review_no                                            Reviews\n",
       "0          0          0  One of the other reviewers has mentioned that ...\n",
       "1          1          0  They are right, as this is exactly what happen...\n",
       "2          2          0  Trust me, this is not a show for the faint hea...\n",
       "3          3          0  This show pulls no punches with regards to dru...\n",
       "4          4          0  Its is hardcore, in the classic use of the wor...\n",
       "5          5          0  It focuses mainly on Emerald City, an experime...\n",
       "6          6          0  Em City is home to many..Aryans, Muslims, gang...\n",
       "7          7          0  Forget pretty pictures painted for mainstream ...\n",
       "8          8          0  The first episode I ever saw struck me as so n...\n",
       "9          9          0  Not just violence, but injustice (crooked guar...\n",
       "10         0          1                     A wonderful little production.\n",
       "11         1          1  <br /><br />The filming technique is very unas...\n",
       "12         2          1  <br /><br />The actors are extremely well chos...\n",
       "13         3          1  You can truly see the seamless editing guided ...\n",
       "14         4          1  A masterful production about one of the great ..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_split.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then normalizing the data\n",
    "data_split['Reviews']=data_split['Reviews'].apply(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>review_no</th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>one reviewers mentioned watching 1 oz episode ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>they right  exactly happened me  br    br   th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>trust  show faint hearted ti amid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>this show pulls punches regards drugs  sex vio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>its hardcore  classic use word  br    br   it ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence  review_no                                            Reviews\n",
       "0         0          0  one reviewers mentioned watching 1 oz episode ...\n",
       "1         1          0  they right  exactly happened me  br    br   th...\n",
       "2         2          0                  trust  show faint hearted ti amid\n",
       "3         3          0  this show pulls punches regards drugs  sex vio...\n",
       "4         4          0  its hardcore  classic use word  br    br   it ..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_split.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.575, subjectivity=0.75)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textblob\n",
    "textblob.TextBlob(\"I hate this film its not good\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "    if data == 2:\n",
    "        return 'positive'\n",
    "    if data == 1:\n",
    "        return 'negative'\n",
    "dataset['sentiment'] = dataset['sentiment'].apply(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: sentiment, dtype: int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "sample = [353,699,900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: This film held my interest enough to watch it several times. The plot has holes, but the lead performers make it work.<br /><br />Catherine Mary Stewart (Julia Kerbridge), does a great job as a woman of 37 who has sacrificed everything else to become a physician. She worked years to earn the money to go to medical school. She is performing brilliantly in her residency and is just about to take her board exam and realize her dream.<br /><br />Meanwhile, Julia's sister and brother-in-law are murdered and as the nearest living relative she is compelled to take in her niece Amanda (Arlen Aguayo-Stewart) to avoid having her become a ward of the state. Amanda is about 7 years old from her appearance. Amanda is so traumatized from her parent's murder that she has become mute. Needless to say, Julia's 16-hour days get longer caring for Amanda.<br /><br />Rob Lowe plays Kevin Finney, a charming neighbor man in their apartment building who works his way into the lives of Julia and Amanda. He is always there with a trick or a joke to help Amanda deal with her distress. Amanda really starts to warm up to Kevin as the film progresses, perhaps more than to her aunt. Julia starts to rely on Kevin to take some of the load of caring for Amanda as she attempts to handle her case load and prepare for her board examination. Kevin is always there whenever some crisis erupts for Julia.<br /><br />The chemistry between Rob and Catherine Mary was great. You keep watching to see them get together before the end of the film. The chemistry between Rob and Arlen was good as well. Arlen managed to convey quite a lot without the benefit of words. The plot had Julia and Amanda gradually warming up to each other. You can see them working out a relationship as the film progresses.<br /><br />We discover that Julia's sister and brother-in-law (the Meyers) were involved in industrial espionage. They stole an extremely valuable prototype microchip from their employer. They had three associates who intended to share the proceeds of the theft. Julia discovers that the Meyers were planning to skip the country under assumed identities. The plot is unclear whether the Meyers intended to double cross their associates or were themselves double crossed.<br /><br />In any case the Meyers are murdered in their home by 2 of their former associates. The killers make no attempt to extract the location of the microchip from the Meyers before killing them. The killers search the home and fail to find their prize. They leave a living witness to their crime, Amanda. The killers then spend the remainder of the film making clumsy attempts to extract the microchip from Julia and Amanda who have no idea where the prize is located. Eventually, the killers kidnap Amanda in hopes she knows something about the microchip's location.<br /><br />Eventually Julia discovers the truth about Kevin. He is an investigator hired to recover the stolen microchip. After some rough moments in the relationship they manage to rescue Amanda and dispatch the bad guys. The predictable ending has the three forming a family and moving happily into a future together.<br /><br />What struck me about the plot were major holes. Kevin moves into the same apartment building as Julia and Amanda the day after the murder of the Meyers. How does he know that the microchip is not already in the hands of the killers? The killers blithely leave fingerprints at the murder scene, with no concern for concealing their identity. The killer that pretends to be a psychiatrist is revealed to Amanda by the remnants of red paint from the murder scene on his shoe soles. He was not shown in the murder scene at the start of the film. There are other weaknesses along these lines too numerous to mention.<br /><br />The film could have been a lot better if the script had been refined more before filming. Filling in the plot holes would have added little expense and greatly improved the effort.\n",
      "SENTIMENT: None\n",
      "Predicted Sentiment polarity: 0.1416666666666667\n",
      "------------------------------------------------------------\n",
      "REVIEW: I decided to watch this movie because I'd not seen Carol Lombard before in any movie. I'm sorry it had to be this one because, quite frankly, this is a dog  and even with Jimmy Stewart and Charles Coburn, both of whom were great actors.<br /><br />The problem with the film is simple: it tries to put too much, too quickly, in to a story about a young lawyer (John Manson played by Stewart) who marries Jane (played by Lombard) within an hour of meeting her. What's that cliché? Marry in haste, repent at leisure... <br /><br />In short, the story is a series of episodes that show the couples' worsening financial status, their troubles with John's live-in mother, their struggles to pay the bills, John's diminished status at the office, the arrival of their baby son, John Jnr (unexpected and causing additional friction at home with mother), the couples' angst about their marriage, the baby's sickness which worsens, thus necessitating an heroic flight by a lone pilot (in a fierce storm) to bring a special serum to save the child, and finally John being accepted as a junior partner at the law firm.<br /><br />How many more clichéd situations could the writers include? Maybe Mother dying soon after? There wasn't much comedy; the drama was lacklustre, at best; the dialog was painful to hear. Only the acting of the four main players was adequate.<br /><br />This was the period at the end of the Great Depression with the USA coming out of its long downturn  during which many people experienced all of the events portrayed in the movie.<br /><br />So, it made sense for Selznick to reaffirm good ol' home spun American values of family, relationships, heroism, perseverance, and initiative  all against the backdrop of the \"average\" American family. Who better to use than Jimmy Stewart and Carol Lombard? <br /><br />And, it should be noted that the film was released in early 1939; so, it was planned in 1938  soon after the USA began to get production going for the coming World War II. Hence, this sort of film was a great booster for the general public, at that time, many of who would soon have to join England in war. As many here would know, Hollywood and Washington formed an uneasy alliance before, during and after the war.<br /><br />However, I'm glad I saw it  as a piece of disguised socio-political propaganda. But, I'll have to see other Lombard films to gain a better appreciation of her acting range.<br /><br />As another reviewer noted: see this one just to say that you've seen all of Stewart's movies; otherwise, don't bother.\n",
      "SENTIMENT: None\n",
      "Predicted Sentiment polarity: 0.23127177700348428\n",
      "------------------------------------------------------------\n",
      "REVIEW: A wonderful film ahead of its time,<br /><br />I think so, In the eighty's it was all about winning, Greed is Good ? Remember that one ? I have seen this film more that 20 times, To me this is a real desert island film, I keep watching because there is always something more to learn about these flawed characters that I just love, Jessica Tandy, and Hume Cronin, are simply wonderful,Also Beverly D'angelo, Beau Bridges come in at a close second, don't get me wrong there are many more great performance's in this film, and it is also the way it is written that made it for me, and I hope you, a film that you will want to see over and over, I think TV shows like \"Northen Exposure\", and now \"Earl\" owe a lot to this film. but remember it is not a Tom Cruise film.\n",
      "SENTIMENT: None\n",
      "Predicted Sentiment polarity: 0.33571428571428574\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(reviews[sample],sentiments[sample]):\n",
    "    print(\"REVIEW:\",review)\n",
    "    print(\"SENTIMENT:\",sentiment)\n",
    "    print('Predicted Sentiment polarity:',textblob.TextBlob(review).sentiment.polarity)\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_polarity = [textblob.TextBlob(review).sentiment.polarity for review in reviews]\n",
    "predicted_sentiments = ['positive' if score >= 0.1 else 'negative' for score in sentiment_polarity]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using AFINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "afn = Afinn(emoticons = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: This film held my interest enough to watch it several times. The plot has holes, but the lead performers make it work.<br /><br />Catherine Mary Stewart (Julia Kerbridge), does a great job as a woman of 37 who has sacrificed everything else to become a physician. She worked years to earn the money to go to medical school. She is performing brilliantly in her residency and is just about to take her board exam and realize her dream.<br /><br />Meanwhile, Julia's sister and brother-in-law are murdered and as the nearest living relative she is compelled to take in her niece Amanda (Arlen Aguayo-Stewart) to avoid having her become a ward of the state. Amanda is about 7 years old from her appearance. Amanda is so traumatized from her parent's murder that she has become mute. Needless to say, Julia's 16-hour days get longer caring for Amanda.<br /><br />Rob Lowe plays Kevin Finney, a charming neighbor man in their apartment building who works his way into the lives of Julia and Amanda. He is always there with a trick or a joke to help Amanda deal with her distress. Amanda really starts to warm up to Kevin as the film progresses, perhaps more than to her aunt. Julia starts to rely on Kevin to take some of the load of caring for Amanda as she attempts to handle her case load and prepare for her board examination. Kevin is always there whenever some crisis erupts for Julia.<br /><br />The chemistry between Rob and Catherine Mary was great. You keep watching to see them get together before the end of the film. The chemistry between Rob and Arlen was good as well. Arlen managed to convey quite a lot without the benefit of words. The plot had Julia and Amanda gradually warming up to each other. You can see them working out a relationship as the film progresses.<br /><br />We discover that Julia's sister and brother-in-law (the Meyers) were involved in industrial espionage. They stole an extremely valuable prototype microchip from their employer. They had three associates who intended to share the proceeds of the theft. Julia discovers that the Meyers were planning to skip the country under assumed identities. The plot is unclear whether the Meyers intended to double cross their associates or were themselves double crossed.<br /><br />In any case the Meyers are murdered in their home by 2 of their former associates. The killers make no attempt to extract the location of the microchip from the Meyers before killing them. The killers search the home and fail to find their prize. They leave a living witness to their crime, Amanda. The killers then spend the remainder of the film making clumsy attempts to extract the microchip from Julia and Amanda who have no idea where the prize is located. Eventually, the killers kidnap Amanda in hopes she knows something about the microchip's location.<br /><br />Eventually Julia discovers the truth about Kevin. He is an investigator hired to recover the stolen microchip. After some rough moments in the relationship they manage to rescue Amanda and dispatch the bad guys. The predictable ending has the three forming a family and moving happily into a future together.<br /><br />What struck me about the plot were major holes. Kevin moves into the same apartment building as Julia and Amanda the day after the murder of the Meyers. How does he know that the microchip is not already in the hands of the killers? The killers blithely leave fingerprints at the murder scene, with no concern for concealing their identity. The killer that pretends to be a psychiatrist is revealed to Amanda by the remnants of red paint from the murder scene on his shoe soles. He was not shown in the murder scene at the start of the film. There are other weaknesses along these lines too numerous to mention.<br /><br />The film could have been a lot better if the script had been refined more before filming. Filling in the plot holes would have added little expense and greatly improved the effort.\n",
      "SENTIMENT: None\n",
      "Predicted Sentiment polarity: -13.0\n",
      "------------------------------------------------------------\n",
      "REVIEW: I decided to watch this movie because I'd not seen Carol Lombard before in any movie. I'm sorry it had to be this one because, quite frankly, this is a dog  and even with Jimmy Stewart and Charles Coburn, both of whom were great actors.<br /><br />The problem with the film is simple: it tries to put too much, too quickly, in to a story about a young lawyer (John Manson played by Stewart) who marries Jane (played by Lombard) within an hour of meeting her. What's that cliché? Marry in haste, repent at leisure... <br /><br />In short, the story is a series of episodes that show the couples' worsening financial status, their troubles with John's live-in mother, their struggles to pay the bills, John's diminished status at the office, the arrival of their baby son, John Jnr (unexpected and causing additional friction at home with mother), the couples' angst about their marriage, the baby's sickness which worsens, thus necessitating an heroic flight by a lone pilot (in a fierce storm) to bring a special serum to save the child, and finally John being accepted as a junior partner at the law firm.<br /><br />How many more clichéd situations could the writers include? Maybe Mother dying soon after? There wasn't much comedy; the drama was lacklustre, at best; the dialog was painful to hear. Only the acting of the four main players was adequate.<br /><br />This was the period at the end of the Great Depression with the USA coming out of its long downturn  during which many people experienced all of the events portrayed in the movie.<br /><br />So, it made sense for Selznick to reaffirm good ol' home spun American values of family, relationships, heroism, perseverance, and initiative  all against the backdrop of the \"average\" American family. Who better to use than Jimmy Stewart and Carol Lombard? <br /><br />And, it should be noted that the film was released in early 1939; so, it was planned in 1938  soon after the USA began to get production going for the coming World War II. Hence, this sort of film was a great booster for the general public, at that time, many of who would soon have to join England in war. As many here would know, Hollywood and Washington formed an uneasy alliance before, during and after the war.<br /><br />However, I'm glad I saw it  as a piece of disguised socio-political propaganda. But, I'll have to see other Lombard films to gain a better appreciation of her acting range.<br /><br />As another reviewer noted: see this one just to say that you've seen all of Stewart's movies; otherwise, don't bother.\n",
      "SENTIMENT: None\n",
      "Predicted Sentiment polarity: 1.0\n",
      "------------------------------------------------------------\n",
      "REVIEW: A wonderful film ahead of its time,<br /><br />I think so, In the eighty's it was all about winning, Greed is Good ? Remember that one ? I have seen this film more that 20 times, To me this is a real desert island film, I keep watching because there is always something more to learn about these flawed characters that I just love, Jessica Tandy, and Hume Cronin, are simply wonderful,Also Beverly D'angelo, Beau Bridges come in at a close second, don't get me wrong there are many more great performance's in this film, and it is also the way it is written that made it for me, and I hope you, a film that you will want to see over and over, I think TV shows like \"Northen Exposure\", and now \"Earl\" owe a lot to this film. but remember it is not a Tom Cruise film.\n",
      "SENTIMENT: None\n",
      "Predicted Sentiment polarity: 18.0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(reviews[sample],sentiments[sample]):\n",
    "    print(\"REVIEW:\",review)\n",
    "    print(\"SENTIMENT:\",sentiment)\n",
    "    print('Predicted Sentiment polarity:',afn.score(review))\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_polarity = [textblob.TextBlob(review).sentiment.polarity for review in reviews]\n",
    "predicted_sentiments = ['positive' if score >= 0.2 else 'negative' for score in sentiment_polarity]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting with Logistic Regression and Decision Tree models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      "review       50000 non-null object\n",
      "sentiment    50000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"D:\\\\sasi\\\\study\\\\ML\\\\ML project\\\\imdb-dataset-of-50k-movie-reviews\\\\IMDB Dataset.csv\")\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split=data.set_index('INDEX').split.apply(pd.Series).stack().reset_index(level=0).rename(columns={0:'Reviews'})\n",
    "data_split.reset_index(level=0,inplace=True)\n",
    "data_split.rename(columns={'INDEX':'review_no','index':'sentence'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data_split['Reviews'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 536641/536641 [04:57<00:00, 1803.54it/s]\n"
     ]
    }
   ],
   "source": [
    "#then normalizing the data\n",
    "normalize_corpus=[]\n",
    "normalize_corpus.append(corpus_pre_processor(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_reviews, test_reviews, train_sentiments, test_sentiments = train_test_split(\n",
    "    reviews,sentiments,test_size=1/3,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing ito train and test\n",
    "reviews= dataset['review'].values\n",
    "sentiments = dataset['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 33333/33333 [00:52<00:00, 634.21it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 16667/16667 [00:27<00:00, 596.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "norm_train_reviews = pre_process_corpus(train_reviews)\n",
    "norm_test_reviews = pre_process_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#biuld BOW\n",
    "cv = CountVectorizer(binary = False, min_df = 5, max_df = 1.0 , ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(norm_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_test_features = cv.transform(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sasim\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(penalty='l2',max_iter=500,C=1,solver='lbfgs')\n",
    "lr.fit (cv_train_features,train_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions = lr.predict(cv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9042"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_sentiments,lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.90      0.90      7490\n",
      "    positive       0.90      0.90      0.90      7510\n",
      "\n",
      "    accuracy                           0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "labels = ['negative','positive']\n",
    "print(classification_report(test_sentiments,lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>negative</td>\n",
       "      <td>6769</td>\n",
       "      <td>721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>positive</td>\n",
       "      <td>716</td>\n",
       "      <td>6794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive\n",
       "negative      6769       721\n",
       "positive       716      6794"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['negative','positive']\n",
    "pd.DataFrame(confusion_matrix(test_sentiments,lr_predictions),index = labels,columns = labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(use_idf =True, min_df=5,max_df=1.0,ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 35000/35000 [00:53<00:00, 654.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 15000/15000 [00:22<00:00, 659.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "norm_train_reviews = pre_process_corpus(train_reviews)\n",
    "norm_test_reviews = pre_process_corpus(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_train_features = tv.fit_transform(norm_train_reviews)\n",
    "tv_test_features = tv.transform(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(tv_train_features,train_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions = lr.predict(cv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.76      0.83      7490\n",
      "    positive       0.79      0.92      0.85      7510\n",
      "\n",
      "    accuracy                           0.84     15000\n",
      "   macro avg       0.85      0.84      0.84     15000\n",
      "weighted avg       0.85      0.84      0.84     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "labels = ['negative','positive']\n",
    "print(classification_report(test_sentiments,lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>negative</td>\n",
       "      <td>5684</td>\n",
       "      <td>1806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>positive</td>\n",
       "      <td>581</td>\n",
       "      <td>6929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive\n",
       "negative      5684      1806\n",
       "positive       581      6929"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['negative','positive']\n",
    "pd.DataFrame(confusion_matrix(test_sentiments,lr_predictions),index = labels,columns = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8408666666666667"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_sentiments,lr_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing ito train and test\n",
    "reviews= dataset['review'].values\n",
    "sentiments = dataset['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_reviewsDT, test_reviewsDT, train_sentimentsDT, test_sentimentsDT = train_test_split(\n",
    "    reviews,sentiments,test_size=1/3,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviewsDT=reviews[:35000]\n",
    "train_sentimentsDT = sentiments[:35000]\n",
    "\n",
    "test_reviewsDT = reviews[35000:]\n",
    "test_sentimentsDT = sentiments[35000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 33333/33333 [00:50<00:00, 660.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 16667/16667 [00:25<00:00, 655.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "norm_train_reviewsDT = pre_process_corpus(train_reviewsDT)\n",
    "norm_test_reviewsDT = pre_process_corpus(test_reviewsDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#biuld BOW\n",
    "cv = CountVectorizer(binary = False, min_df = 5, max_df = 1.0 , ngram_range=(1,2))\n",
    "cv_train_featuresDT = cv.fit_transform(norm_train_reviewsDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_test_featuresDT = cv.transform(norm_test_reviewsDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "lr_D=DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_D.fit(cv_train_featuresDT,train_sentimentsDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictionsDT = lr_D.predict(cv_test_featuresDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.72      0.72      8367\n",
      "    positive       0.71      0.71      0.71      8300\n",
      "\n",
      "    accuracy                           0.72     16667\n",
      "   macro avg       0.72      0.72      0.72     16667\n",
      "weighted avg       0.72      0.72      0.72     16667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "labels = ['negative','positive']\n",
    "print(classification_report(test_sentimentsDT,lr_predictionsDT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>negative</td>\n",
       "      <td>5995</td>\n",
       "      <td>2372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>positive</td>\n",
       "      <td>2370</td>\n",
       "      <td>5930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive\n",
       "negative      5995      2372\n",
       "positive      2370      5930"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['negative','positive']\n",
    "pd.DataFrame(confusion_matrix(test_sentimentsDT,lr_predictionsDT),index = labels,columns = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7154856902861942"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_sentimentsDT,lr_predictionsDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for TFIFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(use_idf =True, min_df=5,max_df=1.0,ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 33333/33333 [00:48<00:00, 681.43it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 16667/16667 [00:25<00:00, 649.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "norm_train_reviewsDT_T = pre_process_corpus(train_reviewsDT)\n",
    "norm_test_reviewsDT_T = pre_process_corpus(test_reviewsDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_train_featuresDT = tv.fit_transform(norm_train_reviewsDT_T)\n",
    "tv_test_featuresDT = tv.transform(norm_test_reviewsDT_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_D.fit(tv_train_featuresDT,train_sentimentsDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictionsDT = lr_D.predict(tv_test_featuresDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.71      0.71      8367\n",
      "    positive       0.71      0.72      0.71      8300\n",
      "\n",
      "    accuracy                           0.71     16667\n",
      "   macro avg       0.71      0.71      0.71     16667\n",
      "weighted avg       0.71      0.71      0.71     16667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "labels = ['negative','positive']\n",
    "print(classification_report(test_sentimentsDT,lr_predictionsDT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>negative</td>\n",
       "      <td>5904</td>\n",
       "      <td>2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>positive</td>\n",
       "      <td>2323</td>\n",
       "      <td>5977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive\n",
       "negative      5904      2463\n",
       "positive      2323      5977"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['negative','positive']\n",
    "pd.DataFrame(confusion_matrix(test_sentimentsDT,lr_predictionsDT),index = labels,columns = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7128457430851383"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_sentimentsDT,lr_predictionsDT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
